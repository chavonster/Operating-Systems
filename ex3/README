marina92, tomer.levy1
Marina Furen (319216438), Tomer Levy (304854938)
EX: 3

FILES:
~~~~~~~~~~~~~~~~~~
README 
Makefile
MapReduceFramework.cpp
Search.cpp

REMARKS:
~~~~~~~~~~~~~~~~~~~
The Mapreduce Framework design is based on the ex description mostly, and we
did not have many design calls in this part. For releasing the memory of k2, v2
pairs we have used 2 vectors which store the pointers for these objects. We
scan the 2 vectors and release memory before the completion of the framework in
case the user does not release the memory by himself.

As for the client implementation. We have chosen to give most of the
functionality to the mapper threads. Conceptually this seemed like a more
appropriate approach even though efficiently speaking we believe it wouldnt
make a big difference. And so each Map function takes a path of directory lists
its content and for every match invokes emit2. Shuffle and reduce
functionality in our design is limited to re-arrange the pairs in the final
output.

ANSWERS:
~~~~~~~~~~~~~~~~~~~~

Part 2: Theoretical Question

First question:
In a conditional variable driven approach we would suggest the following
alternative design: The CR would be a boolean indicating whether or new data to
work on was generated by producers (execMap threads in our case). This variable
would be the condition for a code section inside the consumers job (Shuffle
thread in our case). It will busy wait until producer created more data, this
will trigger shuffle with pthread_cond_timedwait(). Needless to say the CR will
be read and written only after caller has acquired the compatible mutex for it.
The loop mentioned above will be performed as long as there are producers in
the game. Once they are all done we can call consumer for the last time and
then terminate. The reason for using pthread_cond_timedwait() and not
pthread_cond_wait() is to avoid a deadlock situation in the case where consumer
is stuck on busy wait while all producers have completed their work. In
addition, when the locking mutex phase in one of the producers could last a
long time and we do not want to be left busy-waiting during it.  Hence, the
limitation on its waiting time. A pseudo code for this design could be for
instance the following:

Producer thread:

mutexLock()
Emit2()
pthread_cord_signal()
mutexUnlock()

Main thread:

If (allProducersDone())
mutexLock()
producersDone = true
mutexUnlock()

Consumer thread:
While (!producersDone) 
mutexLock()
pthread_cond_timedwait()
crunch_data()
prdocuersDone = allProducersDone()
mutexUnlock()

Second question:
Under the assumption that no other processes are running, if we would want to
maximize the utility of our framework we would want each core to execute a
different thread concurrently. In our implementation we notice that no more
than 2 + multithread level are
running simultaneously for both the map phase and reduce phase. Thus, we would
want to keep all cores busy - if we would choose multiThread level of 6 we
notice we achieve this goal. This conclusion is derived under the assumption
that there are not many I/O operations. If it is indeed this case we would use
the formula (BT/BS + 1) times ofThreads.
  
Third question:
We would go over each person and note their advantages and disadvantages, and
discuss the overall running time at the bottom:
Nira:
Utilizing multi - cores - no usage here since Nira doesnt exert concurrency
mechanism.
scheduler - no scheduling is needed here since there is no concurrency.
The program runs in a sequential fashion.
Communication - optimal.
Ability to progress - Since there is only one path of execution, if it crashed
the entire program crashes.
Overall speed - depends on the amount of data to be crunched - will be
discussed in further detail at the bottom.
Moti:
Utilizing multi - cores - By using the POSIX library we assume the scheduling
is done by the OS in a very efficient manner.
scheduler - scheduling is done by OS thus it is not in our control, Moti could
only decide which threads will be running in a given time. (As we have done in
this exercise).
Communication time - Less overhead time due to the shared data between threads
as opposed to processes.
Ability to progress - if a thread stops functions it will not lead to a total
crash of the program. Only this threads job will terminate.
Overall speed - Due to the shared data the overhead here is low as mentioned.
But multiple race condition scenarios force us to protect the data with
mutexes. This, as we have learned, impacts efficiency.
Dany:
Utilizing multi - cores - User level threads are in fact a pseudo concurrency
implementation. The OS does not actually recognize the usage of multiple
threads. Hence it exerts only one core of the processor.
scheduler - Scheduling is now on the users end. He can use any of the
algorithms we have learned depending on the problem.
Communication time - User level threads have a shared data, hence as in the
kernelsâ€™ thread communication time is relatively good.
Ability to progress - Since the OS is not aware of the multithreading here one
error in a thread results in a collapse of the entire app.
Galit:
Utilizing multi-cores - Multiprocessing allows one to utilise multi-cores since
the OS is responsible for scheduling, similar to kernel threads.
scheduler - Is done by the OS.
Communication time - interprocess communication is the most expensive as we
have learned because they do not share any information. If it wasnt for shared
data structures writing to the same containers by the different processes could
have been quite expensive in overhead time.
Ability to progress - Different processes are independent, hence the collapse
of one does not affect the other whatsoever.
Overall speed - On the one hand as mentioned above no shared data structures
increases overhead. On the other hand, no need to protect them using mutexes
and this compensates on the former. Relatively overall speed is not bad.
Overall speed in general - no method described above is optimal in every case.
The circumstances and conditions matter. For instance the size of data to be
crunched. If the entire job to be does not require extended data crunching then
probably no concurrency at all serves better. Spawning threads as discussed in
class takes time. Time that could have been enough for completing the job. For
bigger jobs concurrency is preferable. User threads have low overhead but
suffer from big disadvantages as not being able to utilize the multi-core and
being unstable in the sense that the crash of one leads to a total collapse of
the program. Multiple processes are protected from these cons but have the
worst communication time. Kernel threads attempt utilize real concurrency and
have lower overhead than processes, but they are not as protected as
multiprocessing.
		
Fourth Question:
User level threads - have the same heap and global variables. Each has a
different stack pointer.
Kernel level threads - Same as user level threads. They share their parent
process heap and globals. Each has a unique stack.
Processes - each process is completely independent of its counterparts, thus
has a unique stack, heap and global variables.

Fifth question:
A deadlock is a situation in which two or more processes/threads are unable to
proceed because each is waiting for one the others to do something. For
example, let us look at two threads, t1 and t2. Let us assume that each process
needs access to two resources in order to perform its job, we would denote
them with r1, r2. A possible execution path if measures are not taken to
prevent it is that each thread receives only one of the resources but not both.
Each thread is waiting for one of the two resources. Neither will release the
resource that it already owns until it has acquired the other resource and
performed the function requiring both resources. Thus, the two threads are
deadlocked.
A livelock is a situation in which two or more threads/processes continuously
deprive themselves of vital resources and thus none of the threads/processes
can proceed with its execution. We would give a real world example for this
case because we believe it fits the definition precisely. Two drives a trying
to cross a crossroad. They both halt in front of the entrance to the junction.
Each driver wants to be kind and let the other enter it first. If none of the
conditions change, In this scenario both drives will not be able to cross and
wont reach their destination on time.
